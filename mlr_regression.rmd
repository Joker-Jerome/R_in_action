---
title: "mlr_regression"
output: html_document
author: Jerome Yu
---

# load libraries 
```{r}
install.packages("mlr")
library(mlr)
```

# load data
```{r}
train <- read.csv("/Users/jerome/Data/train_u6lujuX_CVtuZ9i.csv", na.strings = c(""," ",NA))
test <- read.csv("/Users/jerome/Data/test_Y3wMUE5_7gLdaTN.csv", na.strings = c(""," ",NA))
```


# explore the data
```{r}
summarizeColumns(train)
summarizeColumns(test)
```

# explort the variables
```{r}
# hist
hist(train$ApplicantIncome, breaks = 300, main = "Applicant Income Chart",xlab = "ApplicantIncome")
hist(train$CoapplicantIncome, breaks = 100,main = "Coapplicant Income Chart",xlab = "CoapplicantIncome")

# boxplot
boxplot(train$ApplicantIncome)
```

# check the categorical variables
```{r}
train$Credit_History <- as.factor(train$Credit_History)
test$Credit_History <- as.factor(test$Credit_History)

class(train$Credit_History)

```

# summary again 
```{r}
summary(train)
summary(test)
```

# impute
```{r}
imp <- impute(train, classes = list(factor = imputeMode(), integer = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
imp1 <- impute(test, classes = list(factor = imputeMode(), integer = imputeMean()), dummy.classes = c("integer","factor"), dummy.type = "numeric")
imp_train <- imp$data
imp_test <- imp1$data

```

# Some models which don't need imputation and can handle missing values
```{r}
listLearners("classif", check.packages = TRUE, properties = "missings")[c("class","package")]

```

# Feature engineering
```{r}
# cap large values
# for train data set (maybe we can use pipe operation)
cd <- capLargeValues(imp_train, target = "Loan_Status",cols = c("ApplicantIncome"),threshold = 40000)
cd <- capLargeValues(cd, target = "Loan_Status",cols = c("CoapplicantIncome"),threshold = 21000)
cd <- capLargeValues(cd, target = "Loan_Status",cols = c("LoanAmount"),threshold = 520)
cd_train <- cd
```

```{r}

# add a dummy Loan_Status column in test data
imp_test$Loan_Status <- sample(0:1,size = 367,replace = T)

# for the test set
cde <- capLargeValues(imp_test, target = "Loan_Status",cols = c("ApplicantIncome"),threshold = 33000)
cde <- capLargeValues(cde, target = "Loan_Status",cols = c("CoapplicantIncome"),threshold = 16000)
cde <- capLargeValues(cde, target = "Loan_Status",cols = c("LoanAmount"),threshold = 470)
cd_test <- cde
```

```{r}
#c onvert numeric to factor - train
for (f in names(cd_train[, c(14:20)])) {
    if( class(cd_train[, c(14:20)] [[f]]) == "numeric"){
        levels <- unique(cd_train[, c(14:20)][[f]])
        cd_train[, c(14:20)][[f]] <- as.factor(factor(cd_train[, c(14:20)][[f]], levels = levels))
    }
}

# convert numeric to factor - test
for (f in names(cd_test[, c(13:18)])) {
    if( class(cd_test[, c(13:18)] [[f]]) == "numeric"){
        levels <- unique(cd_test[, c(13:18)][[f]])
        cd_test[, c(13:18)][[f]] <- as.factor(factor(cd_test[, c(13:18)][[f]], levels = levels))
    }
}


```

# Create new values
```{r}
#Total_Income
cd_train$Total_Income <- cd_train$ApplicantIncome + cd_train$CoapplicantIncome
cd_test$Total_Income <- cd_test$ApplicantIncome + cd_test$CoapplicantIncome

#Income by loan
cd_train$Income_by_loan <- cd_train$Total_Income/cd_train$LoanAmount
cd_test$Income_by_loan <- cd_test$Total_Income/cd_test$LoanAmount

#change variable class 
cd_train$Loan_Amount_Term <- as.numeric(cd_train$Loan_Amount_Term)
cd_test$Loan_Amount_Term <- as.numeric(cd_test$Loan_Amount_Term)

#Loan amount by term
cd_train$Loan_amount_by_term <- cd_train$LoanAmount/cd_train$Loan_Amount_Term
cd_test$Loan_amount_by_term <- cd_test$LoanAmount/cd_test$Loan_Amount_Term


```


```{r}
# splitting the data based on class
az <- split(names(cd_train), sapply(cd_train, function(x){ class(x)}))
#creating a data frame of numeric variables
xs <- cd_train[az$numeric]

cor(xs)
```

# remove features which have strong correlations
```{r}
cd_train$Total_Income <- NULL
cd_test$Total_Income <- NULL
```


# machine learning
```{r}
# create a task user friendly
trainTask <- makeClassifTask(dasta = cd_train,target = "Loan_Status")
testTask <- makeClassifTask(data = cd_test, target = "Loan_Status")
```


```{r}
trainTask <- makeClassifTask(data = cd_train,target = "Loan_Status", positive = "Y")
```


```{r} 

# pathlogy, cancer 
# Until here, we’ve performed all the important transformation steps except normalizing the skewed variables. That will be done after we create the task.

erainTask <- makeClassifTask(data = cd_train,target = "Loan_Status")
testTask <- makeClassifTask(data = cd_test, target = "Loan_Status")

```
# discounted


Now, we will normalize the data. 
For this step, we’ll use normalizeFeatures function from mlr package.
normalize the variables
trainTask <- normalizeFeatures(trainTask,method = "standardize")
testTask <- normalizeFeatures(testTask,method = "standardize")



```{r}
trainTask <- dropFeatures(task = trainTask,features = c("Loan_ID","Married.dummy"))
# #Feature importance
im_feat <- generateFilterValuesData(trainTask, method = c("information.gain","chi.squared"))
plotFilterValues(im_feat,n.show = 20)
}
```


```{r}
#to launch its shiny application
plotFilterValuesGGVIS(im_feat)
```

# Some models
```{r}
# logistic.learner <- makeLearner("classif.logreg",predict.type = "response")
logistic.learner <- makeLearner("classif.logreg",predict.type = "response")


# cv.logistic <- crossval(learner = logistic.learner,task = trainTask,iters = 3,stratify = TRUE,measures = acc,show.info = F)

```

```{r}
cv.logistic <- crossval(learner = logistic.learner,task = trainTask,iters = 3,stratify = TRUE,measures = acc,show.info = F)

cv.logistic <- crossval(learner = logistic.learner,task = trainTask,iters = 3,stratify = TRUE,measures = acc,show.info = F)


cv.logistic$aggr

```

```{r}
cv.logistic$measures.test
```



```{r}

fmodel <- train(logistic.learner,trainTask)
getLearnerModel(fmodel)


#predict on test data
fpmodel <- predict(fmodel, testTask)

#create submission file
submit <- data.frame(Loan_ID = test$Loan_ID, Loan_Status = fpmodel$data$response)
write.csv(submit, "submit2.csv",row.names = F)

```