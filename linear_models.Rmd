---
title: "linear_model"
author: "Jerome"
date: "1/24/2021"
output: html_document
---

# http://rebeccaferrell.github.io/CSSS508/Homework/template-HW4-key.html


# linear models 
```{r}
set.seed(1234)
n <- 2000 # number of UW calc I students

# simulate high school math GPA
hs_math_gpa <- rnorm(n, mean = 3.5, sd = 0.5)
# truncate to 2.0-4.0
hs_math_gpa <- ifelse(hs_math_gpa < 2.0, 2.0,
                      ifelse(hs_math_gpa > 4.0, 4.0, hs_math_gpa))

# simulate previous calculus in high school:
# - 75% of people with HS math GPAs over 3.6 took HS calculus
# - 40% of people with HS math GPAs under 3.6 took HS calculus
# binomial data with 1 trial per student, probabilities as above
hs_calculus <- rbinom(n, size = 1,
                      p = ifelse(hs_math_gpa >= 3.6, 0.75, 0.40))

# simulate precalculus at UW:
# - if no high school calculus, 70% take precalculus, regardless of grade
# - if high school calculus, 60% take precalculus if HS grade < 3.5,
#   and 25% otherwise
uw_precalc <- rbinom(n, size = 1,
                     p = ifelse(hs_calculus == 0, 0.70,
                                ifelse(hs_math_gpa < 3.5, 0.60, 0.25)))
```

```{r}
true_beta <- c("Intercept" = 0.3,
               "hs_math_gpa" = 0.7,
               "hs_calculus" = 0.3,
               "uw_precalc" = 0.1)
true_sigma <- 0.5
uw_calculus_gpa <- true_beta["Intercept"] +
    true_beta["hs_math_gpa"] * hs_math_gpa +
    true_beta["hs_calculus"] * hs_calculus +
    true_beta["uw_precalc"] * uw_precalc +
    rnorm(n, mean = 0, sd = true_sigma)
# we don't see exact GPAs, just nearest 0.1
uw_calculus_gpa <- round(uw_calculus_gpa, 1)
# truncate under 0.7 or over 4.0
uw_calculus_gpa <- ifelse(uw_calculus_gpa > 4.0, 4.0,
                          ifelse(uw_calculus_gpa < 0.0, 0.0,
                                 ifelse(uw_calculus_gpa < 0.7, 0.7,
                                        uw_calculus_gpa)))
```

```{r}
calculus_data <- data.frame(uw_calculus_gpa, hs_math_gpa, hs_calculus, uw_precalc)
head(calculus_data)
```

```{r}
# this uses the lm function seen in Week 1
calculus_lm <- lm(uw_calculus_gpa ~ hs_math_gpa + hs_calculus + uw_precalc,
                  data = calculus_data) 
summary(calculus_lm)
```

```{r}
# to make X, just cbind the variables together in the same
# order as the independent variables in the regression
# (so everthing matches up)
X <- cbind(1, hs_math_gpa, hs_calculus, uw_precalc)
# the colnames for the existing variables are already okay
# but I still want to name the intercept column
colnames(X)[1] <- "Intercept"
colnames(X)
```

```{r}
y <- uw_calculus_gpa


# t(X) is the transponse, %*% is matrix multiplication,
# solve takes the inverse
A <- solve(t(X) %*% X)
# alternative solution: crossprod does the same thing
A <- solve(crossprod(X))

# this comes from looking at formula for beta hat and
# multiplying the additional terms needed
beta <- A %*% t(X) %*% y

# just filling in formula as above
residuals <- y - X %*% beta


p <- ncol(X) - 1

residual_var <- t(residuals) %*% residuals / (n - p - 1)
residual_var <- as.numeric(residual_var)

# alternative calculation # 1:
residual_var <- crossprod(residuals) / (n - p - 1)
residual_var <- as.numeric(residual_var)

# alternative calculation # 2:
residual_var <- sum(residuals^2) / (n - p - 1)


```


```{r}
head(beta)
```


```{r}

# covariance matrix of estimated regression coefficients
# is just the estimated residual variance times solve(t(X) %*% X)
# we calculuted earlier and stored as A
beta_covar <- residual_var * A


# diag takes the diagonal of the matrix, sqrt makes it go
# from variance to standard deviation
beta_SE <- sqrt(diag(beta_covar))
```


# comparison 
```{r}
# true covariance uses same formula as estimated covariance
# except we plug in the true variance. we have the true std deviation
# so we square that
library(pander)
true_covar <- true_sigma^2 * A
true_SE <- sqrt(diag(true_covar))
SE_compare <- cbind(true_SE,
                    beta_SE,
                    summary(calculus_lm)[["coefficients"]][, "Std. Error"])
colnames(SE_compare) <- c("Truth", "Manual", "lm")
pander(SE_compare, caption = "Comparison of standard errors of linear regression parameters estimated manually with those from R's lm().")
```

```{r}
resid_var_compare <- cbind(true_sigma^2,
                           residual_var,
                           summary(calculus_lm)[["sigma"]]^2)
colnames(resid_var_compare) <- c("Truth", "Manual", "lm")
pander(resid_var_compare, caption = "Comparison of residual variances of manual linear regression with that from R's lm().")
```

# t stat significance 
```{r}
t_stat = beta/beta_SE
head(t_stat)
```

# p value by hand 
```{r}
pt(t_stat, df = (n - p - 1), lower.tail = F)*2
```

# logistic regression 
```{r}
# data generation 
set.seed(2016)
# simulate data 
# independent variables
x1 = rnorm(30,3,2) + 0.1*c(1:30)
x2 = rbinom(30, 1,0.3)
x3 = rpois(n = 30, lambda = 4)
x3[16:30] = x3[16:30] - rpois(n = 15, lambda = 2)

# dependent variable 
y = c(rbinom(5, 1,0.1),rbinom(10, 1,0.25),rbinom(10, 1,0.75),rbinom(5, 1,0.9))
```

```{r}
# disign matrix 
x0 = rep(1,30) #bias
X = cbind(x0,x1,x2,x3)
```

```{r}
manual_logistic_regression = function(X,y,threshold = 1e-10, max_iter = 1000)
  #A function to find logistic regression coeffiecients 
  #Takes three inputs: 
{
  #A function to return p, given X and beta
  #We'll need this function in the iterative section
  calc_p = function(X,beta)
  {
    beta = as.vector(beta)
    return(exp(X%*%beta) / (1+ exp(X%*%beta)))
  }  

  #### setup bit ####

  #initial guess for beta
  beta = rep(0, ncol(X))

  #initial value bigger than threshold so that we can enter our while loop 
  diff = 10000 

  #counter to ensure we're not stuck in an infinite loop
  iter_count = 0
  
  #### iterative bit ####
  while(diff > threshold ) #tests for convergence
  {
    #calculate probabilities using current estimate of beta
    p = as.vector(calc_p(X, beta))
    
    #calculate matrix of weights W
    W =  diag(p*(1-p)) 
    
    #calculate the change in beta
    beta_change = solve(t(X) %*% W %*% X) %*% t(X) %*% (y - p)
    
    #update beta
    beta = beta + beta_change
    
    #calculate how much we changed beta by in this iteration 
    #if this is less than threshold, we'll break the while loop 
    diff = sum(beta_change^2)
    
    #see if we've hit the maximum number of iterations
    iter_count = iter_count + 1
    if(iter_count > max_iter) {
      stop("This isn't converging, mate.")
    }
  }
  #make it pretty 
  coef = c("(Intercept)" = beta[1], x1 = beta[2], x2 = beta[3], x3 = beta[4])
  return(coef)
}
```

```{r}
#using R package 
M1 = glm(y~x1+x2+x3, family = "binomial")
M1$coefficients
# (Intercept)          x1          x2          x3 
# -1.3512086   0.3191309   0.2033449  -0.0832102 

# our version
coef = manual_logistic_regression(X,y)
coef
# (Intercept)          x1          x2          x3 
# -1.3512086   0.3191309   0.2033449  -0.0832102 

#they're the same! Amazing!
```

# se estimate and significance
```{r}
calc_p = function(X, beta) {
    beta = as.vector(beta)
    return(exp(X%*%beta) / (1+ exp(X%*%beta)))
}
p = as.vector(calc_p(X, coef))
se = sqrt(diag(solve(t(X) %*% diag(p*(1-p)) %*% X)))


# weights
(p*(1-p))
as.vector(M1$weights)

# sd
se
summary(M1)$coef[,2]

# z value 
(coef/se)
summary(M1)$coef[,3]

# p value 
p_raw = pnorm(coef/se, lower.tail = F)
2*ifelse(p_raw < 0.5, p_raw, 1-p_raw)
summary(M1)$coef[,4]



```

