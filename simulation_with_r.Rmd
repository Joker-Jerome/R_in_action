---
title: "intro_simulation_r"
author: "Jerome"
date: "11/15/2020"
output: html_document
---


## Introductory Examples

Example 1.1. Sampling Computer Chips. Suppose there are 100 memory chips in a box, of which 90 are “good” and 10 are “bad.” We withdraw five of the 100 chips at random to upgrade a computer. What is the probability that all five chips are good?
```{r}
# 1 combination 
choose(90, 5)/choose(100, 5)

# 2 
n_sim = 10000
res_vec = rep(NA, n_sim)
res_vec = numeric(n_sim)
for (i in 1:n_sim) {
    tmp_s = sample(1:100, 5, replace = F)
    res_vec[i] = sum(tmp_s <= 90)
    
}
mean(res_vec == 5)
```

To implement sampling with replacement, one would have to include the argument repl=T. But in taking chips from the box, we want to sample without replacement. This is the default sampling mode in R, so the effect
is the same whether we include or omit the argument repl=F.


Example 1.2. Birthday Matches—Combinatorial Approach. Suppose there are n = 25 people in a room. What is the probability that two or more of them have the same birthday? This is an intriguing problem because many people expect the probability to be a lot smaller than it is.

```{r}
n = 60
p = numeric(n)
for (i in 1:n) {
    
  #q = prod(1 - (0:(i-1))/365)
    #365! / (365-n)!  / 365^n
    #365 * 364 .... (365-n+1)
    #365 * 365 ...  365
    q = prod(1 - (0:(i-1))*(1/365))
        #q =  (factorial(365)/factorial(365-n))/365^n
    p[i] = 1 - q
# vector of room sizes
# initialize vector, all 0s
# index values for loop
# P(No match) if i people in room
# changes ith element of p
# plot of p against n
}
plot(1:n, p)
```

Example 1.3. Birthday Matches—Using Simulation. Again, we focus attention on a room with n = 25 randomly chosen people and assume there are 365 equally likely birthdays in a year. We begin by simulating the birthdays in one room. Numbering the days of the year from 1 to 365, we can use the sample function to get a list of 25 random birthdays in a room. Of course, we sample with replacement here because we need to allow for the possibility of matching birthdays. One use of this function with the appropriate parameters gives the following result:

```{r}
set.seed(1237)
m = 100000
n = 25
x = numeric(m)
for (i in 1:m)
{
  b = sample(1:365, n, repl=T)
  x[i] = n - length(unique(b))
}
mean(x == 0)
mean(x)
cutp = (0:(max(x)+1)) - .5
hist(x, breaks=cutp, prob=T)
# iterations; people in room
# vector for numbers of matches
# n random birthdays in ith room
# no. of matches in ith room
# approximates P{X=0}; E(X)
# break points for histogram
# relative freq. histogram
```
# what about specific number of matches like 2 
c(365, 24) * 24 * p(24, 23)/365^n 
```{r}

```

Example 1.4. Estimating the Probability that a Die Shows a Six. As an ele- mentary illustration of confidence intervals made with formula (1.2), suppose 20 students in a class were each asked to roll a die 30 times. We note the number X of 6s observed and find the corresponding confidence interval.


```{r}
set.seed(0)
n_sim = 1000
res = numeric(n_sim)
for (i in 1:n_sim) {
    res[i] = sample(1:6, 1)
}
mean(res == 6)
z = qnorm(0.975)
mean_vec + z_

```

# sd 
```{r}

n_sim = 1000
res_list = list()

for (i in 1:n_sim) {
    
    res_list[[i]] = sample(1:6, 1000, replace = T)
}

mean_vec = unlist(lapply(res_list, function(x) mean(x == 6)))
the_sd = sqrt(1/6 * (1-1/6)/1000)
sim_sd = sd(mean_vec)

dnorm(0.95)

z = qnorm(0.975)
mean(mean_vec) - qnorm(0.975) * the_sd 
mean(mean_vec) + qnorm(0.975) * the_sd 
```



Example 1.5. Two Coverage Probabilities. Suppose a new process for making a prescription drug is in development. Of n = 30 trial batches made with the current version of the process, X = 24 batches give satisfactory results. Then p = 24/30 = 0.8 estimates the population proportion π = P(Success) of satisfactory batches with the current version of the process. Wondering how near p = 0.8 might be to π, the investigators use (1.2) to obtain the approximate 95% confidence interval 0.8 ± 0.143 or (0.657, 0.943).



```{r}
                                          # number of trials
                                          # n+1 possible outcomes
                                          # n+1 Margins of error
                                          # n+1 Lower conf. limits
n = 30
x=0:n
sp=x/n
m.err = 1.96*sqrt(sp*(1-sp)/n)
lcl = sp - m.err
ucl = sp + m.err
                        # n+1 Upper conf. limits
pp=.80
prob = dbinom(x, n, pp)
cover = (pp >= lcl) & (pp <= ucl)
round(cbind(x, sp, lcl, ucl, prob, cover), 4) # 4-place printout sum(dbinom(x[cover], n, pp)) # total cov. prob. at pp

```

Example 1.6. Two Thousand Coverage Probabilities. To get a more compre- hensive view of the performance of confidence intervals based on formula (1.2), we step through two thousand values of π from near 0 to near 1. For each value of π, we go through a procedure like that shown in Example 1.5. Finally, we plot the coverage probabilities against π.


```{r}
n = 30
alpha = .05;  k = qnorm(1-alpha/2)
adj = 0
x=0:n
sp=(x+adj)/(n+2*adj) 
m.err = k*sqrt(sp*(1 - sp)/(n + 2*adj))
lcl = sp - m.err
ucl = sp + m.err

m = 2000
pp = seq(1/n, 1 - 1/n, length=m)
p.cov = numeric(m)
for (i in 1:m)
{
# number of trials
# conf level = 1-alpha
# (2 for Agresti-Coull)
     cover = (pp[i] >= lcl) & (pp[i] <= ucl)   #  1 if cover, else 0
     p.rel = dbinom(x[cover], n, pp[i])        #  relevant probs.
     p.cov[i] = sum(p.rel)                     #  total coverage prob.
   }
   plot(pp, p.cov, type="l", ylim=c(1-4*alpha,1))
   lines(c(.01,.99), c(1-alpha,1-alpha))

```


## Generating Random Numbers
r_i+1 = a*r_i + b (mod d),



Example 2.1. As a “toy” illustration, suppose we want to shuffle the 52 cards in a standard deck. For convenience, number them 1, 2, 3, . . . , 52. Then consider the generator with a = 20, b = 0, d = 53, and s = 21. To obtain its first 60 numbers, we treat R simply as a programming language, using the following
```{r}
d = 53      # modulus
a = 20      # multiplier
b = 0       # shift
s = 21      # seed
m = 60      # length of run 
r = numeric(m) 
r[1] = s
for (i in 1:(m-1)) {
    r[i+1] = (a * r[i] + b) %% d
}
                # generates random integers
r                # list of random integers generated
```

We set the d as 53 so that the algo can have the full period since there are 52 possible values coming out of the calculation. 


Example 2.2. Here we consider a generator that is “pretty good,” but not really good enough for serious modern simulations. Let a = 1093, b = 18 257, d = 86 436, and s = 7. In elementary software applications, this generator has the advantage that the arithmetic involved never produces a number larger than 108 (see [PTVF92]).

```{r}
# Initialize
a=1093
b=18257 
d=86436
s=7 
m = 1000
r = numeric(m)
r[1] = s
   # Generate
for (i in 1:(m-1)) {r[i+1] = (a*r[i] + b) %% d}
u = (r + 1/2)/d
# Display Results
par(mfrow=c(1,2), pty="s")
hist(u, breaks=10, col="wheat")
abline(h=m/10, lty="dashed")
u1 = u[1:(m-1)]; u2 = u[2:m]
plot (u1, u2, pch=19)
par(mfrow=c(1,1), pty="m")
# values fit in (0,1)
# 2 square panels in a plot
# left panel
# right panel
# return to default
```

Example 2.3. Some years ago, IBM introduced a generator called RANDU for some of its mainframe computers. Essentially a multiplicative generator with a = 65539 and d = 231, it came to be very widely used. But over time it acquired a bad reputation. First, it was found to yield wrong Monte Carlo answers in some computations where the right answers were known by other means. Later, it was found to concentrate its values on a very few planes in 3-dimensional space. 

```{r}
a=65539; b=0; d=2^31; s=10 
m = 20000; r = numeric(m); r[1] = s
for (i in 1:(m-1)) {r[i+1] = (a*r[i] + b) %% d}
   u = (r - 1/2)/(d - 1)
   u1 = u[1:(m-2)]; u2 = u[2:(m-1)]; u3 = u[3:m]
   par(mfrow=c(1,2), pty="s")
     plot(u1, u2, pch=19, xlim=c(0,.1), ylim=c(0,.1))
     plot(u1[u3 < .01], u2[u3 < .01], pch=19, xlim=c(0,1), ylim=c(0,1))
   par(mfrow=c(1,1), pty="m")
```

runif 

The Mersenne twister has period 219937 − 1 ≈ 4.32 × 106001, and it has been tested for good behavior in up to 623 consecutive dimensions. These distinct generated values are mapped into the roughly 4.31 billion numbers that can be expressed within the precision of R

```{r}

```



2.4 Transformations of Uniform Random Variables

```{r}
# to simulate a roll of two fair dice
# 1
ceiling(runif(2, 0, 6))
# 2 
sample(1:6, 2, rep=T)


```

Example 2.4. The Square of a Uniform Random Variable. Let U ∼ UNIF(0, 1). We seek the distribution of X = U^2. The support of a continuous random variable is the part of the real line for which its density function is positive. So the support of U is the unit interval (0, 1). Because the square of a number in the unit interval is again in the unit interval, it seems clear that the support of X is also (0,1).
```{r}
# set.seed(1234)
   m = 10000
   u = runif(m);  x = u^2
   xx = seq(0, 1, by=.001) # python np.linsaple
   cut.u = (0:10)/10;  cut.x = cut.u^2
   par(mfrow=c(1,2))
     hist(u, breaks=cut.u, prob=T, ylim=c(0,10))
       lines(xx, dunif(xx), col="blue")
     hist(x, breaks=cut.x, prob=T, ylim=c(0,10))
       lines(xx, .5*xx^-.5, col="blue")
   par(mfrow=c(1,1))
```

The density function of X can be derived using an argument based on its cumulative distribution function (CDF). Recall that the CDF of U is given by FU (u) = P {U ≤ u} = u, for 0 < u < 1. Then, for 0 < x < 1, we have
FX(x)=P{X ≤x}=P{U2 ≤x}=P{U ≤x1/2}=x1/2.

Taking the derivative of this CDF to get the density function of X, we obtain fX(x) = 0.5x−0.5, which is plotted along with the histogram of simulated values of X in Figure 2.5. This is the density function of BETA(0.5, 1). 

In Example 2.4, we have seen how to find the density function of a trans-
formed standard uniform random variable. Now suppose we want to simulate
random samples from a particular distribution. Then the question is how to
find the transformation that will change a standard uniform distribution into
the desired one. In Example 2.4, notice that the CDF of X ∼ BETA(0.5, 1) is
FX(x) = x1/2, for 0 < x < 1, that the quantile function (inverse CDF) of X
is x = F−1(u) = u2, and that X = F−1(U) = U2 is the transformation we XX
used to generate X ∼ BETA(0.5, 1).


This result for BETA(0.5, 1) is not a coincidence. In general, if FY is the
CDF of a continuous random variable, then F −1 (U ) has the distribution of Y . Y
This is called the quantile transformation method of simulating a distri- bution.



```{r}

sum_vec = rep(NA, 100) 
for (x in 1:100) {
  n_sample = 100
  vec = rep(NA, n_sample)
  for (i in 1:n_sample) {
    vec[i] = sum(sample(c(1,2,3,4,5,6), 100, replace = T))
  }
  sum_vec[x] = var(vec)
}
  


```

